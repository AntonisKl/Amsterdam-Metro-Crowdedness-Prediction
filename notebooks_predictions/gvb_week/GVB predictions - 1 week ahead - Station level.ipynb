{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GVB predictions - 1-week ahead - Station level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To-do:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* env file needs to be loaded. Note that \"from sqlalchemy import create_engine\" and \"import env\" needs to be uncommented in the third cell.\n",
    "\n",
    "* Predictions need to be stored in the database.\n",
    "\n",
    "* Decide where to calculate crowd levels. Do this based on static csv from GVB.\n",
    "\n",
    "* Events are specified manually and are taken from a static file at this moment. As a temporary solution, we can save events in the database. Ideally, we need to get events from an API.\n",
    "\n",
    "* Historical weather data and weather forecast have different units for some columns? This is currently solved by multiplying everything by 10, which should be correct.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "get_ipython().run_cell_magic('bash', '', 'pip install psycopg2-binary\\npip install workalendar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "#from sqlalchemy import create_engine\n",
    "#import env\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import substring, length, col, expr\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import requests\n",
    "\n",
    "from datetime import datetime, timedelta, date\n",
    "import time\n",
    "import pytz\n",
    "from workalendar.europe import Netherlands\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_error\n",
    "\n",
    "import helpers_gvb as h\n",
    "\n",
    "import importlib   # to reload helpers without restarting kernel: importlib.reload(h)\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create engine for SQL queries\n",
    "#engine = create_engine(\"postgresql://{}:{}@{}:{}/{}\".format(env.DATABASE_USERNAME_AZ, \n",
    "#                                                            env.DATABASE_PASSWORD_AZ, \n",
    "#                                                            \"igordb.postgres.database.azure.com\", \n",
    "#                                                            5432, \n",
    "#                                                            \"igor\"),\n",
    "#                       connect_args={'sslmode':'require'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stations to create predictions for\n",
    "stations = ['Centraal Station', 'Station Zuid', 'Station Bijlmer ArenA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = pd.to_datetime(\"today\")\n",
    "today_str = str(today.year) + \"-\" + str(today.month) + \"-\" + str(today.day)\n",
    "covid_url = 'https://covidtrackerapi.bsg.ox.ac.uk/api/v2/stringency/date-range/2020-1-1/' + today_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Start loading raw data') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 2019 GVB data (TEMPORARILY)\n",
    "gvb_2019_bestemming_raw = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"s3a://gvb-gvb/topics/gvb/2020/04/20/Datalab_Reis_Bestemming_Uur_2019.csv\", sep = \";\").toPandas()\n",
    "gvb_2019_herkomst_raw = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"s3a://gvb-gvb/topics/gvb/2020/04/20/Datalab_Reis_Herkomst_Uur_2019.csv\", sep = \";\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 2020 GVB data (TEMPORARILY)\n",
    "gvb_2020_bestemming_raw = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"s3a://gvb-gvb/topics/gvb/2020/*/*/Datalab_Reis_Bestemming_Uur_2020*.csv\", sep = \";\").toPandas()\n",
    "gvb_2020_herkomst_raw = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"s3a://gvb-gvb/topics/gvb/2020/*/*/Datalab_Reis_Herkomst_Uur_2020*.csv\", sep = \";\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 2021 GVB data (csv format only) (TEMPORARILY)\n",
    "gvb_2021_bestemming_raw = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"s3a://gvb-gvb/topics/gvb/2021/*/*/Datalab_Reis_Bestemming_Uur_2021*.csv\", sep = \";\").toPandas()\n",
    "gvb_2021_herkomst_raw = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"s3a://gvb-gvb/topics/gvb/2021/*/*/Datalab_Reis_Herkomst_Uur_2021*.csv\", sep = \";\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge data from above 3 cells (TEMPORARILY)\n",
    "gvb_bestemming_raw_csv = pd.concat([gvb_2019_bestemming_raw, gvb_2020_bestemming_raw, gvb_2021_bestemming_raw])\n",
    "gvb_herkomst_raw_csv = pd.concat([gvb_2019_herkomst_raw, gvb_2020_herkomst_raw, gvb_2021_herkomst_raw])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GVB data in JSON format\n",
    "gvb_bestemming_raw_json = spark.read.format(\"json\").option(\"header\", \"true\").load(\"s3a://gvb-gvb/topics/gvb/*/*/*/*.json.gz\", sep = \",\").toPandas()\n",
    "gvb_herkomst_raw_json = spark.read.format(\"json\").option(\"header\", \"true\").load(\"s3a://gvb-gvb/topics/gvb-herkomst/*/*/*/*.json.gz\", sep = \",\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load weather data\n",
    "knmi_obs = spark.read.format(\"json\").load(\"s3a://knmi-knmi/topics/knmi-observations/*/*/*/*\").toPandas()\n",
    "knmi_pred = spark.read.format(\"json\").option(\"header\", \"true\").load(\"s3a://knmi-knmi/topics/knmi/2021/*/*/*.json.gz\", sep = \";\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_df_raw = pd.DataFrame(requests.get(url = covid_url).json()['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holidays_data_raw = Netherlands().holidays(2019) + Netherlands().holidays(2020) + Netherlands().holidays(2021) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vacations_df = h.get_vacations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = h.get_events()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-process data sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Start pre-processing data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestemming, herkomst = h.merge_csv_json(gvb_bestemming_raw_csv, gvb_herkomst_raw_csv, gvb_bestemming_raw_json, gvb_herkomst_raw_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast 'AantalReizen' to int to sum up\n",
    "bestemming['AantalReizen'] = bestemming['AantalReizen'].astype(int)\n",
    "herkomst['AantalReizen'] = herkomst['AantalReizen'].astype(int)\n",
    "\n",
    "# Remove all duplicates\n",
    "bestemming.drop_duplicates(inplace=True)\n",
    "herkomst.drop_duplicates(inplace=True)\n",
    "\n",
    "# Group by station name because we are analysing per station\n",
    "bestemming_grouped = bestemming.groupby(['Datum', 'UurgroepOmschrijving (van aankomst)', 'AankomstHalteNaam'], as_index=False)['AantalReizen'].sum()\n",
    "herkomst_grouped = herkomst.groupby(['Datum', 'UurgroepOmschrijving (van vertrek)', 'VertrekHalteNaam'], as_index=False)['AantalReizen'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestemming_herkomst = h.merge_bestemming_herkomst(bestemming_grouped, herkomst_grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gvb_dfs = []\n",
    "\n",
    "for station in stations:\n",
    "    gvb_dfs.append(h.preprocess_gvb_data_for_modelling(bestemming_herkomst, station))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knmi_historical = h.preprocess_knmi_data_hour(knmi_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knmi_forecast = h.preprocess_metpre_data(knmi_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_df = h.preprocess_covid_data(covid_df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "holiday_df = h.preprocess_holiday_data(holidays_data_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge datasources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gvb_dfs_merged = []\n",
    "\n",
    "for df in gvb_dfs:\n",
    "    gvb_dfs_merged.append(merge_gvb_with_datasources(df, knmi_historical, covid_df, holiday_df, vacations_df, events))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Start cleaning data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpolate missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gvb_dfs_interpolated = []\n",
    "\n",
    "for df in gvb_dfs_merged:\n",
    "    gvb_dfs_interpolated.append(h.interpolate_missing_values(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gvb_dfs_final = []\n",
    "\n",
    "for df in gvb_dfs_interpolated:\n",
    "    df['check-ins'] = df['check-ins'].astype(int)\n",
    "    df['check-outs'] = df['check-outs'].astype(int)\n",
    "    df[['check-ins_week_ago', 'check-outs_week_ago']] = df.apply(lambda x: h.get_crowd_last_week(df, x), axis=1, result_type=\"expand\")\n",
    "    gvb_dfs_final.append(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Create model dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_splits = []\n",
    "\n",
    "for df in gvb_dfs_final:\n",
    "    train, validation, test = h.get_train_val_test_split(df.dropna())\n",
    "    data_splits.append([train, validation, test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and targets. This is the same for all stations at the moment.\n",
    "features = ['year', 'month', 'weekday', 'hour', 'holiday', 'vacation', 'planned_event', 'check-ins_week_ago', \n",
    "            'check-outs_week_ago', 'stringency', 'temperature', 'wind_speed', 'precipitation_h']\n",
    "targets = ['check-ins', 'check-outs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_splits = []\n",
    "y_train_splits = []\n",
    "\n",
    "X_validation_splits = []\n",
    "y_validation_splits = []\n",
    "\n",
    "X_test_splits = []\n",
    "y_test_splits = []\n",
    "\n",
    "for split in data_splits:\n",
    "    X_train_splits.append(split[0][features])\n",
    "    y_train_splits.append(split[0][targets])\n",
    "    \n",
    "    X_validation_splits.append(split[1][features])\n",
    "    y_validation_splits.append(split[1][targets])\n",
    "    \n",
    "    X_test_splits.append(split[2][features])\n",
    "    y_test_splits.append(split[2][targets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframes to predict check-ins and check-outs of next week\n",
    "X_predict_dfs = []\n",
    "\n",
    "for df in gvb_dfs_final:\n",
    "    X_predict_dfs.append(h.get_future_df(features, df, covid_df.tail(1)['stringency'][0], holiday_df, vacations_df, knmi_forecast, events))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Start modelling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_models = []\n",
    "\n",
    "for x in range(0, len(data_splits)):\n",
    "    model_basic, r_squared_basic, mae_basic, rmse_basic = h.train_random_forest_regressor(X_train_splits[x], y_train_splits[x], \n",
    "                                                                                          X_validation_splits[x], y_validation_splits[x], \n",
    "                                                                                          None)\n",
    "    basic_models.append([model_basic, r_squared_basic, mae_basic, rmse_basic])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tune (hyper-)parameters (not done because models currently do not improve with hyperparameter tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify hyperparameters, these could be station-specific. For now, default hyperparameter settings are being used.\n",
    "centraal_station_hyperparameters = None\n",
    "station_zuid_hyperparameters = None\n",
    "station_bijlmer_arena_hyperparameters = None\n",
    "\n",
    "hyperparameters = [centraal_station_hyperparameters,\n",
    "                   station_zuid_hyperparameters, \n",
    "                   station_bijlmer_arena_hyperparameters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tuned_models = []\n",
    "\n",
    "#for x in range(0, len(data_splits)):\n",
    "#    model_tuned, r_squared_tuned, mae_tuned, rmse_tuned = h.train_random_forest_regressor(X_train_splits[x], y_train_splits[x], \n",
    "#                                                                                          X_validation_splits[x], y_validation_splits[x], \n",
    "#                                                                                          hyperparameters[x])\n",
    "#    tuned_models.append([model_tuned, r_squared_tuned, mae_tuned, rmse_tuned])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Improvements compared to basic model (negative is worse performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for x in range(0, len(basic_models)):\n",
    "#    print(\"R-squared difference\", tuned_models[x][1]-basic_models[x][1])\n",
    "#    print(\"MAE difference\", tuned_models[x][2]-basic_models[x][2])\n",
    "#    print(\"RMSE difference\", tuned_models[x][3]-basic_models[x][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train test model (including validation data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_models = []\n",
    "\n",
    "for x in range(0, len(data_splits)):\n",
    "    X_train_with_val = pd.concat([X_train_splits[x], X_validation_splits[x]])\n",
    "    y_train_with_val = pd.concat([y_train_splits[x], y_validation_splits[x]])\n",
    "    \n",
    "    model_test, r_squared_test, mae_test, rmse_test = h.train_random_forest_regressor(X_train_with_val, y_train_with_val, \n",
    "                                                                                          X_test_splits[x], y_test_splits[x], \n",
    "                                                                                          hyperparameters[x])\n",
    "    test_models.append([model_test, r_squared_test, mae_test, rmse_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check models on R-squared score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(0, len(test_models)):\n",
    "    station_name = stations[x]\n",
    "    r_squared = test_models[x][1]\n",
    "    if r_squared < 0.7:\n",
    "        warnings.warn(\"Model for \" + station_name + \" shows unexpected performance!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train final models (to make predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_models = []\n",
    "\n",
    "for x in range(0, len(data_splits)):\n",
    "    X_train_with_val = pd.concat([X_train_splits[x], X_validation_splits[x], X_test_splits[x]])\n",
    "    y_train_with_val = pd.concat([y_train_splits[x], y_validation_splits[x], y_test_splits[x]])\n",
    "    \n",
    "    model_final = h.train_random_forest_regressor(X_train_with_val, y_train_with_val, X_test_splits[x], y_test_splits[x], \n",
    "                                                  hyperparameters[x])[0]\n",
    "    final_models.append(model_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Prepare output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Start preparing data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "\n",
    "for predict_df in X_predict_dfs:\n",
    "    for model in final_models:\n",
    "        prediction = h.predict(model, predict_df)\n",
    "        predictions.append(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate crowd levels here? Or use trigger in database?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Store data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Start storing data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(0, len(stations)):\n",
    "    station_name = stations[x] # Use this to write predictions to database\n",
    "    predictions_for_station = predictions[x]\n",
    "    \n",
    "    final_prediction_dataframe = predictions_for_station.drop(columns=['year', 'month', 'weekday', 'holiday', 'vacation', \n",
    "                                                                       'planned_event', 'check-ins_week_ago', \n",
    "                                                                       'check-outs_week_ago', 'stringency', 'temperature', \n",
    "                                                                       'wind_speed', 'precipitation_h'])\n",
    "    \n",
    "    # Add time to datetime column so that it is easier to store in database\n",
    "    final_prediction_dataframe['datetime'] = final_prediction_dataframe.apply(lambda x: x['datetime'].replace(hour=x['hour']), axis=1)\n",
    "    final_prediction_dataframe.drop(columns=['hour'], inplace=True)\n",
    "    final_prediction_dataframe['Station'] = station_name\n",
    "\n",
    "    ### Code to write data to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Finished storing data')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
