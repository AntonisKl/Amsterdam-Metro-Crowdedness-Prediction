{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do's:\n",
    "- When there is a larger set of data with weather predictions (after 2021/06/15), remove weather observations and switch to only using weather predictions\n",
    "- Update list of locations to use in report\n",
    "- Improve/validate prediction model based on above changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resono 1 week predictions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make 1 week-ahead predictions of the visitor counts (hourly visitor counts based on historic data) for all locations (in druktebeeld) or a list of Resono locations. \n",
    "\n",
    "Generate a graph with the predictions for in the weekly report for each location. The graph will be automatically saved in the directory that you can set in the arguments section. \n",
    "\n",
    "Predictions are stored in a data frame with the following additional columns: \n",
    "- **'total_count_predicted'**: predicted total counts (for the next 7 days per location)\n",
    "- **'data_version'**: version of the data (feature set)\n",
    "- **'model_version'**: version of the model (type and settings)\n",
    "- **'predicted_at'**: timestamp of prediction (moment prediction was made)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Information for when using this notebook:\n",
    "\n",
    "Data file needed: \n",
    "- Hourly total counts of historic Resono data (retrieved from Resono dashboard)\n",
    "\n",
    "Current model:\n",
    "- Linear regression (based on validation with 7 weeks for selection of locations, against baseline model (repeat past week))\n",
    "\n",
    "Model input current version:\n",
    "- Past observations Resono data (average past few weeks etc.)\n",
    "- Periodic data (time of day etc.)\n",
    "- Stringency Index\n",
    "- Holiday data\n",
    "- Vacation data\n",
    "- Weather data **observations** (< 2021/06/15), and **predictions** (>= 2021/06/15) (temperature, wind speed, global radiation, cloud cover)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparations\n",
    "\n",
    "Change directory to folder that contains the function files/database credentials in code blocks below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def install_packages():\n",
    "    # (Re-)Installs packages.\n",
    "    \n",
    "    get_ipython().run_cell_magic('bash', '', 'pip install imblearn\\npip install xgboost\\npip install mord\\npip install psycopg2-binary\\npip install workalendar\\npip install eli5\\n pip install plotly')\n",
    "    \n",
    "    import pandas as pd\n",
    "    pd.set_option('mode.chained_assignment', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "install_packages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install scikit-learn==0.24.2  # Run if sklearn error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/opt/spark-3.0.3-bin-without-hadoop/jars/slf4j-log4j12-1.7.26.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/opt/hadoop-3.2.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
      "Warning: Ignoring non-Spark config property: com.amazonaws.services.s3.enableV4\n",
      "Warning: Ignoring non-Spark config property: mapreduce.fileoutputcommitter.algorithm.version\n",
      "2021-11-18 18:19:51,501 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from datetime import timedelta\n",
    "\n",
    "#os.chdir(\"/home/jovyan/Credentials\") # Directory with Azure DB credentials\n",
    "import env_az\n",
    "\n",
    "#os.chdir(\"/home/jovyan/gitops/central_storage_analyses/notebooks_predictions/resono_week\")\n",
    "import prediction_model_helpers as h  # Universal predictions\n",
    "import resono_week_predictions as resono_pred  # Resono 1 week model specific\n",
    "\n",
    "import importlib  # For when coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Arguments for functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file name of historic Resono data (total daily counts)\n",
    "resono_data_dir = \"data/\"\n",
    "file_name = '2021-01-01_2021-11-11_totalsperhour.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency of sampling for data source to predict\n",
    "freq = 'H'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many samples in a day\n",
    "n_samples_day = 24\n",
    "# how many samples in a week\n",
    "n_samples_week = 24*7\n",
    "# what period to predict for operational forecast (samples)\n",
    "predict_period = n_samples_week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of column name(s) of variabe to predict (can also be \"all\")\n",
    "#Y_names = \"all\" \n",
    "#Y_names = ['Albert Cuyp', 'Vondelpark West', 'Rembrandtplein',\n",
    "#          'Nieuwmarkt', 'Leidseplein', 'Kalverstraat Noord', 'Kalverstraat Zuid']\n",
    "\n",
    "Y_names = ['Albert Cuyp','Vondelpark West']\n",
    "\n",
    "# data source (for which the predictions are made)\n",
    "data_source = 'resono'\n",
    "\n",
    "# type of prediction (count -> regression or level -> classification)\n",
    "target = 'count'  # Can only be count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input for starting of learnset \n",
    "start_learnset = h.get_start_learnset(train_length = 20, date_str = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input for start prediction\n",
    "start_prediction = \"2021-11-15 00:00:00\"  # start date of week to predict \n",
    "start_prediction = pd.to_datetime(start_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimum number of training samples needed to make predictions (otherwise no predictions for that location)\n",
    "min_train_samples = n_samples_week*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform outlier removal (\"yes\" or \"no\")\n",
    "outlier_removal = \"no\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set versions (for storing results)\n",
    "current_model_version = 'lr_0_0'\n",
    "current_data_version = \"1_0\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report graph settings\n",
    "report_dir = \"/\"\n",
    "week_label = 46\n",
    "legend = \"yes\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Prepare data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o50.load.\n: java.nio.file.AccessDeniedException: s3a://knmi-knmi/topics/knmi-observations/2021/06/*/*: getFileStatus on s3a://knmi-knmi/topics/knmi-observations/2021/06/*/*: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: 16B8B6FD7E1ED3EE; S3 Extended Request ID: null), S3 Extended Request ID: null:403 Forbidden\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:230)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:151)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:2275)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:2226)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:2160)\n\tat org.apache.hadoop.fs.FileSystem.isDirectory(FileSystem.java:1707)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.isDirectory(S3AFileSystem.java:3044)\n\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:47)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:376)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:297)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:286)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:286)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:232)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: 16B8B6FD7E1ED3EE; S3 Extended Request ID: null), S3 Extended Request ID: null\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1712)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1367)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1113)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:770)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:744)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:726)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:686)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:668)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:532)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:512)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4920)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4866)\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1320)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$4(S3AFileSystem.java:1307)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:322)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:285)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:1304)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:2264)\n\t... 22 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_976/2069190129.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m base_df, resono_df, resono_df_raw, start_prediction, end_prediction, Y_names_all = resono_pred.prepare_data(env_az,\n\u001b[0m\u001b[1;32m      4\u001b[0m                                                                                                             \u001b[0mresono_data_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                                                                                             \u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Predictions/Resono weekly/resono_week_predictions.py\u001b[0m in \u001b[0;36mprepare_data\u001b[0;34m(env, resono_data_dir, file_name, freq, predict_period, start_prediction, n_samples_day, Y_names, target, start_learnset)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m     \u001b[0mknmi_obs_minio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"header\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"s3a://knmi-knmi/topics/knmi-observations/2021/06/*/*\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\";\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mknmi_obs_schema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0mknmi_obs_df_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mknmi_obs_minio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0mknmi_pred_minio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_knmi_preds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o50.load.\n: java.nio.file.AccessDeniedException: s3a://knmi-knmi/topics/knmi-observations/2021/06/*/*: getFileStatus on s3a://knmi-knmi/topics/knmi-observations/2021/06/*/*: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: 16B8B6FD7E1ED3EE; S3 Extended Request ID: null), S3 Extended Request ID: null:403 Forbidden\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:230)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:151)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:2275)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:2226)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:2160)\n\tat org.apache.hadoop.fs.FileSystem.isDirectory(FileSystem.java:1707)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.isDirectory(S3AFileSystem.java:3044)\n\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:47)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:376)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:297)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:286)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:286)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:232)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: com.amazonaws.services.s3.model.AmazonS3Exception: Forbidden (Service: Amazon S3; Status Code: 403; Error Code: 403 Forbidden; Request ID: 16B8B6FD7E1ED3EE; S3 Extended Request ID: null), S3 Extended Request ID: null\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1712)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1367)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1113)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:770)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:744)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:726)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:686)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:668)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:532)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:512)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4920)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:4866)\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1320)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$4(S3AFileSystem.java:1307)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:322)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:285)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:1304)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:2264)\n\t... 22 more\n"
     ]
    }
   ],
   "source": [
    "t1 = time()\n",
    "\n",
    "base_df, resono_df, resono_df_raw, start_prediction, end_prediction, Y_names_all = resono_pred.prepare_data(env_az,\n",
    "                                                                                                            resono_data_dir,\n",
    "                                                                                                            file_name,\n",
    "                                                                                                           freq, \n",
    "                                                                                                           predict_period, \n",
    "                                                                                                           start_prediction,\n",
    "                                                                                                            n_samples_day, \n",
    "                                                                                                           Y_names, \n",
    "                                                                                                           target,\n",
    "                                                                                                           start_learnset)\n",
    "\n",
    "t2 = time()\n",
    "\n",
    "print('Completed in %s sec.' % (str(t2 - t1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Make predictions and store in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_976/1977575922.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Initialize data frame with predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mfinal_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mgbl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# --- remove in version without backtesting\n",
    "prepared_dfs = dict()\n",
    "y_scalers = dict()\n",
    "# ---\n",
    "\n",
    "# Initialize data frame with predictions\n",
    "final_df = pd.DataFrame()\n",
    "\n",
    "gbl = globals()\n",
    "\n",
    "# Predict for each location\n",
    "for idx, Y in enumerate(Y_names_all):\n",
    "    \n",
    "    # Show location\n",
    "    print(Y)\n",
    "    \n",
    "    # Preprocessed data frame for this location\n",
    "    preprocessed_df = resono_pred.get_location_df(base_df, resono_df, Y)\n",
    "    \n",
    "    # Gather predictons for this location\n",
    "    prepared_df, predictions, y_scaler = resono_pred.get_resono_predictions(preprocessed_df, resono_df_raw, freq, predict_period, n_samples_day, \n",
    "                                                             n_samples_week, Y, data_source, target, \n",
    "                                                             outlier_removal, start_learnset,\n",
    "                                                             current_model_version, current_data_version, \n",
    "                                                             start_prediction, end_prediction, min_train_samples)\n",
    "    # Add predictions to final data frame\n",
    "    gbl['final_df'+'_'+str(Y)] = pd.concat([final_df, predictions], 0)\n",
    "    \n",
    "    # Get and store report figure\n",
    "    gbl['report_df'+'_'+str(Y)] = resono_pred.get_location_report_df(gbl['final_df'+'_'+str(Y)], prepared_df, y_scaler, Y)\n",
    "    resono_pred.get_report_plot_hourly(gbl['report_df'+'_'+str(Y)], legend, Y, report_dir, week_label)\n",
    "    #resono_pred.get_report_plot_daily(report_df, Y, report_dir, week_label)\n",
    "    \n",
    "    # --- remove in version without backtesting\n",
    "    gbl['prepared_dfs'+'_'+str(Y)] = prepared_df\n",
    "    y_scalers[Y] = y_scaler\n",
    "    # ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Plot prediction graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_report_plot_daily(report_df, Y_name, report_dir, week_label):\n",
    "    '''\n",
    "    Get a graph of the predictions compared to past observations on a daily rate with week and weekend split.\n",
    "    '''\n",
    "    \n",
    "    # Get daily sum and split between weekdays and weekend\n",
    "    daily = report_df.resample('D').sum()\n",
    "    daily_weekend = daily[daily.index.weekday.isin([5, 6])]\n",
    "    daily_weekdays = daily[daily.index.weekday.isin([0, 1, 2, 3, 4])]\n",
    "    \n",
    "    prvs_df = preprocessed_df[start_prediction.date() - timedelta(days=21) : start_prediction.date() - timedelta(hours=23)].resample('D').sum()\n",
    "    prvs_df['week'] = prvs_df.index.week\n",
    "    \n",
    "    prvs_weekend = prvs_df[prvs_df.index.weekday.isin([5, 6])]\n",
    "    prvs_weekdays = prvs_df[prvs_df.index.weekday.isin([0, 1, 2, 3, 4])]\n",
    "    \n",
    "    prvs_weekend['week'] = prvs_weekend.index.week\n",
    "    prvs_weekend = prvs_weekend.groupby('week').median().reset_index()\n",
    "    \n",
    "    prvs_weekdays['week'] = prvs_weekdays.index.week\n",
    "    prvs_weekdays = prvs_weekdays.groupby('week').median().reset_index()\n",
    "    \n",
    "    prvs_weekend = prvs_weekend[['week', Y_name]].rename(columns={Y_name:'Count'})\n",
    "    prvs_weekdays = prvs_weekdays[['week', Y_name]].rename(columns={Y_name:'Count'})\n",
    "    \n",
    "    # Get the mean for weekdays/weekend\n",
    "    daily_weekdays_avg = daily_weekdays.mean(axis = 0).to_frame().reset_index()\n",
    "    daily_weekend_avg = daily_weekend.mean(axis = 0).to_frame().reset_index()\n",
    "    \n",
    "    # Rename columns\n",
    "    daily_weekdays_avg.columns = ['week', Y_name]\n",
    "    daily_weekend_avg.columns = ['week', Y_name]\n",
    "    \n",
    "    daily_weekdays_avg = daily_weekdays_avg.rename(columns={Y_name:'Count'})\n",
    "    daily_weekend_avg = daily_weekend_avg.rename(columns={Y_name:'Count'})\n",
    "    \n",
    "    daily_weekdays = pd.concat([prvs_weekdays, daily_weekdays_avg])\n",
    "    daily_weekends = pd.concat([prvs_weekend, daily_weekend_avg])\n",
    "    \n",
    "    daily_weekdays = daily_weekdays.replace({'Voorspelling':week_label})\n",
    "    daily_weekends = daily_weekends.replace({'Voorspelling':week_label})\n",
    "    \n",
    "    daily_weekdays['week'] = daily_weekdays.week.astype(int)\n",
    "    daily_weekends['week'] = daily_weekends.week.astype(int)\n",
    "    \n",
    "    # Determine colors in graph\n",
    "    daily_weekdays['color'] = 'green'\n",
    "    daily_weekdays['color'][daily_weekdays['week'] == week_label] = 'lightgreen'\n",
    "    daily_weekends['color'] = 'green'\n",
    "    daily_weekends['color'][daily_weekends['week'] == week_label] = 'lightgreen'\n",
    "    \n",
    "    # Convert average past 4 weeks to week numbers and prediction to current week number\n",
    "    #daily_weekdays_avg['Week'] = [str(int(week_label)-3) + \"-\" + str(int(week_label)-1), week_label]\n",
    "    #daily_weekend_avg['Week'] = [str(int(week_label)-3) + \"-\" + str(int(week_label)-1), week_label]\n",
    "    \n",
    "    # Parameters\n",
    "    plt.rcParams[\"axes.axisbelow\"] = True\n",
    "    plt.rcParams.update({'axes.titlesize': 14,\n",
    "                     'axes.labelsize': 14, 'xtick.labelsize': 14, 'ytick.labelsize': 14,\n",
    "                     'axes.labelpad': 8.0\n",
    "                    })\n",
    "    \n",
    "    # Initialize figure\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (8,4), dpi = 100, frameon = False, sharey = True, \n",
    "                                     constrained_layout = True)\n",
    "\n",
    "    # Title\n",
    "    fig.suptitle(Y_name, fontsize = 18)\n",
    "    \n",
    "    # Weekdays figure\n",
    "    ax1.bar(x = daily_weekdays['week'], height = daily_weekdays['Count'], color = daily_weekdays['color'])\n",
    "    ax1.set_title('Doordeweeks (ma-vr)')\n",
    "    ax1.set_xlabel(\"Week\")\n",
    "    ax1.set_ylabel(\"Aantal Resono counts per dag\")\n",
    "    ax1.spines['right'].set_visible(False)\n",
    "    ax1.spines['top'].set_visible(False)\n",
    "    ax1.grid(axis = 'y', color = 'lightgrey')\n",
    "\n",
    "    # Weekend figure\n",
    "    ax2.bar(x =  daily_weekends['week'], height = daily_weekends['Count'], color = daily_weekends['color'])\n",
    "    ax2.set_title('Weekend (za-zo)')\n",
    "    ax2.spines['right'].set_visible(False)\n",
    "    ax2.spines['top'].set_visible(False)\n",
    "    ax2.grid(axis = 'y', color = 'lightgrey')\n",
    "    ax2.set_xlabel(\"Week\")\n",
    "\n",
    "    # Store figure\n",
    "    #os.makedirs(report_dir + week_label, exist_ok=True) \n",
    "    #os.chdir(report_dir + week_label)\n",
    "    fig_name = Y_name.replace(\" \", \"_\") + \"_resono_week_ahead_prediction_daily.png\"\n",
    "    plt.savefig(fig_name, bbox_inches='tight', dpi = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_report_plot_daily(report_df, 'Albert Cuyp' , report_dir, week_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Evaluate - Check operational prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backtesting --- remove code blocks below in version without backtesting\n",
    "\n",
    "Test model predictions for the selected location (argument at the beginning) and time period (start_test; within the time period for which the data has been prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input for backtesting\n",
    "\n",
    "# Start testing from this timestamp until the most recent time slot\n",
    "start_test = \"2021-11-01 00:00:00\"\n",
    "# What period to predict for backtesting (samples)\n",
    "predict_period = n_samples_week*7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If using a NN/LSTM model, it is necessary to also install these libraries\n",
    "# Related functions have to be uncommented in prediction_model_helpers.py\n",
    "#pip install keras\n",
    "#pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform backtesting\n",
    "\n",
    "# Store results\n",
    "locations = []\n",
    "rmse_benchmarks = []\n",
    "rmse_models = []\n",
    "figs_pred_time = dict()\n",
    "feat_imps = dict()\n",
    "figs_feat_imp = dict()\n",
    "\n",
    "# Predict for each location\n",
    "for idx, Y in enumerate(Y_names_all):\n",
    "    \n",
    "    # Show location\n",
    "    print(Y)\n",
    "    \n",
    "    # Prepare data\n",
    "    if Y in prepared_dfs:\n",
    "        df_y_predict_bt, df_y_train_bt, df_y_ground_truth_bt, df_y_ground_truth_bt_scaled, df_X_train_bt, df_X_predict_bt = h.prepare_backtesting(start_test, predict_period, freq, \n",
    "                                                                                   prepared_dfs[Y], Y, \n",
    "                                                                                   n_samples_week, target, y_scalers[Y])\n",
    "    \n",
    "    # Do not perform backtesting if there is not enough training data \n",
    "    if (df_X_train_bt.empty) | (len(df_X_train_bt) < min_train_samples):\n",
    "        print(\"Not enough training data: no backtesting performed.\")\n",
    "        continue\n",
    "    \n",
    "    # Benchmark predictions\n",
    "    df_y_benchmark = df_y_predict_bt.copy()\n",
    "    df_y_benchmark[Y] = h.test_model_past_week_bt(df_y_train_bt, df_y_predict_bt, df_y_ground_truth_bt_scaled, \n",
    "                                                    predict_period, \n",
    "                                                   n_samples_week, target)\n",
    "    if target == \"count\":\n",
    "        df_y_benchmark = h.unscale_y(df_y_benchmark, y_scalers[Y])\n",
    "        \n",
    "    error_metrics_benchmark = h.evaluate(df_y_benchmark, df_y_ground_truth_bt, target, Y_name = Y,\n",
    "                                         print_metrics = False)\n",
    "    \n",
    "    rmse_benchmarks.append(error_metrics_benchmark['rmse'])\n",
    "    \n",
    "    # Model predictions\n",
    "    df_y_model = df_y_predict_bt.copy()\n",
    "    \n",
    "    model = h.train_model_ridge_regression(df_X_train_bt, df_y_train_bt, Y, target)\n",
    "    df_y_model[Y] = h.test_model_ridge_regression(model, df_X_predict_bt)\n",
    "    if target == \"count\":\n",
    "        df_y_model = h.unscale_y(df_y_model, y_scalers[Y])\n",
    "    error_metrics_model = h.evaluate(df_y_model, df_y_ground_truth_bt, target, Y_name = Y, print_metrics = False)\n",
    "    \n",
    "    rmse_models.append(error_metrics_model['rmse'])\n",
    "    \n",
    "    # Visualize backtesting result\n",
    "    fig_pred_time = h.visualize_backtesting(df_y_ground_truth_bt, df_y_benchmark, df_y_model, target, Y, \n",
    "                                        error_metrics_model, y_label = \"Total visitor count\", count_to_level = False)\n",
    "    figs_pred_time[Y] = fig_pred_time\n",
    "    \n",
    "    # Feature importance\n",
    "    feat_imp, fig_feat_imp = h.feature_importance(model.coef_[0], list(df_X_train_bt.columns))\n",
    "    feat_imps[Y] = feat_imp\n",
    "    figs_feat_imp[Y] = fig_feat_imp\n",
    "    \n",
    "    locations.append(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backtesting results for all locations\n",
    "df_results = h.backtesting_results_all_locations(locations, rmse_models, rmse_benchmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarized results\n",
    "df_results.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locations for which the model performs better\n",
    "df_results[df_results['RMSE_difference'] < 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locations for which the benchmark model performs better\n",
    "df_results[df_results['RMSE_difference'] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query results for specific location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results[df_results['Location'] == \"Albert Cuyp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figs_pred_time[\"Vondelpark West\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figs_feat_imp[\"Vondelpark West\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
